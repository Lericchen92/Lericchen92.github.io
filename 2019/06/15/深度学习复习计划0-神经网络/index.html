<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">



  
  
  <link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css">










<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=6.4.1" rel="stylesheet" type="text/css">



  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.ico?v=6.4.1">











<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '6.4.1',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;人工神经网络是深度学习的核心，它们通用、强大、可扩展，成为解决大型和复杂机器学习任务的理想选择。">
<meta name="keywords" content="深度学习">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习复习计划0 - 神经网络">
<meta property="og:url" content="http://whatsmore.info/2019/06/15/深度学习复习计划0-神经网络/index.html">
<meta property="og:site_name" content="除此之外">
<meta property="og:description" content="&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;人工神经网络是深度学习的核心，它们通用、强大、可扩展，成为解决大型和复杂机器学习任务的理想选择。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://whatsmore.info/2019/06/15/深度学习复习计划0-神经网络/1-1.png">
<meta property="og:image" content="http://whatsmore.info/2019/06/15/深度学习复习计划0-神经网络/1-2.png">
<meta property="og:image" content="http://whatsmore.info/2019/06/15/深度学习复习计划0-神经网络/1-3.png">
<meta property="og:image" content="http://whatsmore.info/2019/06/15/深度学习复习计划0-神经网络/1-4.png">
<meta property="og:image" content="http://whatsmore.info/2019/06/15/深度学习复习计划0-神经网络/1-5.png">
<meta property="og:updated_time" content="2019-06-29T02:37:25.357Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深度学习复习计划0 - 神经网络">
<meta name="twitter:description" content="&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;人工神经网络是深度学习的核心，它们通用、强大、可扩展，成为解决大型和复杂机器学习任务的理想选择。">
<meta name="twitter:image" content="http://whatsmore.info/2019/06/15/深度学习复习计划0-神经网络/1-1.png">






  <link rel="canonical" href="http://whatsmore.info/2019/06/15/深度学习复习计划0-神经网络/">



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>深度学习复习计划0 - 神经网络 | 除此之外</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">
  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">除此之外</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">Whatsmore?</p>
      
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-about">
    <a href="/about/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
    <a href="/tags/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">
    <a href="/categories/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>
  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://whatsmore.info/2019/06/15/深度学习复习计划0-神经网络/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Shadowlight">
      <meta itemprop="description" content="划水运动员，摸鱼爱好者的乐园。">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="除此之外">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">深度学习复习计划0 - 神经网络
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-15 09:04:00" itemprop="dateCreated datePublished" datetime="2019-06-15T09:04:00+08:00">2019-06-15</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-29 10:37:25" itemprop="dateModified" datetime="2019-06-29T10:37:25+08:00">2019-06-29</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/好好学习/" itemprop="url" rel="index"><span itemprop="name">好好学习</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/06/15/深度学习复习计划0-神经网络/#comments" itemprop="discussionUrl">
                  <span class="post-meta-item-text">评论数：</span> <span class="post-comments-count valine-comment-count" data-xid="/2019/06/15/深度学习复习计划0-神经网络/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2019/06/15/深度学习复习计划0-神经网络/" class="leancloud_visitors" data-flag-title="深度学习复习计划0 - 神经网络">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数：</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>&nbsp;&nbsp;&nbsp;&nbsp;人工神经网络是深度学习的核心，它们通用、强大、可扩展，成为解决大型和复杂机器学习任务的理想选择。</p>
<a id="more"></a>
<h3 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;感知机是最简单的人工神经网络架构之一，神经元接收来自n个其他神经元传输过来的输入信号，并将这些输入值加权求和($z=w_1x_1+w_2x_2+…+w_nx_n=w^T·x$)，再对求值结果进行阈值检查后应用一个激活函数，产生最后的输出：$h_w(x)=f(z)=f(w^T·x - \theta)$。<br>&nbsp;&nbsp;&nbsp;&nbsp;这里的$\theta$指的是阈值，$f$指激活函数，我们选用最简单的阶跃函数，如下图所示:   <img src="/2019/06/15/深度学习复习计划0-神经网络/1-1.png" alt="图片1-1"></p>
<h4 id="学习规则"><a href="#学习规则" class="headerlink" title="学习规则"></a>学习规则</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;神经网络的学习规则，就是调整神经元之间的<strong>连接权重</strong>（表明对特征的关注度）和神经元内部的<strong>阈值</strong>（决定有多少加权和来激活神经元）。<br>&nbsp;&nbsp;&nbsp;&nbsp;假设学习规则是这样的：<br>$$\begin{align}<br>w_{new}&amp;=w_{old}+\epsilon  \\<br>\theta_{new}&amp;=\theta_{old}+\epsilon<br>\end{align}$$<br>&nbsp;&nbsp;&nbsp;&nbsp;其中$\epsilon = y-y’$，$y$是希望输出，$y’$是实际输出，那么$\epsilon$就是两者的差值，而这个差值，就是网络连接权重和阈值的调整动力。感知机就是通过这样反复试错，来找到最终拟合实际输出的权重和阈值的。<br>&nbsp;&nbsp;&nbsp;&nbsp;这里为了表示更加方便，可以将阈值$\theta$视为$w_0$，其输入值固定为1，这样依赖，权重和阈值的学习可以统一成一个权重的学习，从而简化学习规则：<br>$$\begin{align}<br>y&amp;=w_1x_1+w_2x_2+…+w_nx_n-\theta \\<br>&amp;=w_0x_0+w_1x_1+w_2x_2+…+w_nx_n    \\<br>&amp;=\sum_{i=0}^nw_ix_i<br>\end{align}<br>$$<br>&nbsp;&nbsp;&nbsp;&nbsp;在简化之后，感知机的权值调整规则可以统一调整为：<br>$$\begin{align}<br>w_{inew}&amp;=w_{iold}+\Delta w_i  \\<br>\Delta w_i&amp;=\eta(y_i-{y_i}’)x_i<br>\end{align}$$<br>&nbsp;&nbsp;&nbsp;&nbsp;其中$\eta\in(0,1)$称为学习率，可以减缓每一步权重调整的强度，如果太小，可能收敛速度会非常慢；如果太大，会错过网络最优解，因此该值较多依赖于人工经验（属于超参数），通常将其初始值设置为较小的值（如0.1）。</p>
<h4 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;单层神经网络无法解决线性不可分割的问题，如异或分类问题（XOR）等。  </p>
<h3 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;要解决线性不可分问题，就需要让网络复杂起来，因为复杂网络的表征能力比较强，因此可以在输入层和输出层之间添加一层神经元，称其为隐藏层（Hidden Layer），隐藏层和输出层的神经元都有激活函数。事实证明，这种形式的神经网络有效的解决了线性不可分问题，这就是多层感知机（又称前馈神经网络）。  </p>
<h4 id="神经元的几何意义"><a href="#神经元的几何意义" class="headerlink" title="神经元的几何意义"></a>神经元的几何意义</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;对于一个具有$n$个特征的对象$x$，若令学习规则方程为0，则可视为在n维空间的超平面P，那么神经元就可以看作一个由超平面划分空间位置的识别器。<br>&nbsp;&nbsp;&nbsp;&nbsp;举个例子，假设神经元模型构造了$w_1x_1+w_2x_2-\theta=0$的超平面方程，那么其学习方式就是不断调整$w_1,w_2$（旋转超平面）和$\theta$（平移超平面）的值，以尽可能让所有样本的分类都准确。<br>&nbsp;&nbsp;&nbsp;&nbsp;那么如果多添加一个神经元，就相当于新增一个超平面，划分更多子空间。理论上这个过程可以一直下去，直到所有分类都是准确的。    </p>
<h4 id="多层网络的构造方向"><a href="#多层网络的构造方向" class="headerlink" title="多层网络的构造方向"></a>多层网络的构造方向</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;前面提到，神经网络可以通过添加神经元的个数来增加分类精度，这样就会让神经网络变得浅而宽。而另外一个进化方向就是朝着纵深进行，也就是减少隐藏层的神经元数量，增加神经网络的层数，让神经网络向着深而瘦的方向发展。<br>&nbsp;&nbsp;&nbsp;&nbsp;那么哪个方向更好一些呢？研究显示，增加网络的层数会显著提升神经网络系统的学习性能，而网络层数太深的话，BP算法会带来梯度消失问题，从而导致增加层数无法提升性能。因此，深度学习还是应当考虑朝着“纵深方向”发展的。</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;在监督学习算法中，为了衡量实际输出值$\bar Y$与预期值$Y$的差距，需要定义一个<strong>损失函数</strong>（Loss Function）$L(Y,\bar Y)=L(Y,f(X))$来衡量二者之间的差距。<br>&nbsp;&nbsp;&nbsp;&nbsp;在很多地方我们可能也看到过“代价函数（Cost Function）”的描述，吴恩达老师对于这两个概念的定义区别如下：损失函数是针对单个样本来说的，而代价函数是针对全体样本来说的。<br>&nbsp;&nbsp;&nbsp;&nbsp;那么……为什么不最大化正确分类数目，反而要最小化损失函数呢？因为正确分类的数量对参数的调整不敏感，导致很难通过改变权重和偏置来确定正确的优化方向。  </p>
<h4 id="常见的损失函数"><a href="#常见的损失函数" class="headerlink" title="常见的损失函数"></a>常见的损失函数</h4><p>（1）0-1损失函数(0-1 loss function)：<br>$$L(Y, f(X))=\begin{cases}1,&amp;Y\neq f(X)\\0,&amp;Y=f(X)\end{cases}$$<br>&nbsp;&nbsp;&nbsp;&nbsp;也就是说，当预测错误时，损失函数为1，当预测正确时，损失函数值为0。该损失函数不考虑预测值和真实值的误差程度。只要错误，就是1。<br>（2）绝对损失函数(absolute loss function)：<br>$$L(Y,f(X))=|Y-f(X)|$$<br>&nbsp;&nbsp;&nbsp;&nbsp;取绝对值避免负值出现。<br>（3）平方损失函数(quadratic loss function)：<br>$$L(Y,f(X))={\frac{1}{2}(Y-f(x))^2}$$<br>&nbsp;&nbsp;&nbsp;&nbsp;计算预测值与实际值差的平方，前面的$\frac{1}{2}$是为了方便求导。<br>（4）对数损失函数(logarithmic loss function):<br>$$L(y,p(y|x))=−logp(y|x)$$<br>&nbsp;&nbsp;&nbsp;&nbsp;该损失函数用到了极大似然估计的思想。P(Y|X)通俗的解释就是：在当前模型的基础上，对于样本X，其预测值为Y，也就是预测正确的概率。由于概率之间的同时满足需要使用乘法，为了将其转化为加法，我们将其取对数。最后由于是损失函数，所以预测正确的概率越高，其损失值应该是越小，因此再加个负号取个反。</p>
<h4 id="常见的代价函数"><a href="#常见的代价函数" class="headerlink" title="常见的代价函数"></a>常见的代价函数</h4><p>（1）均方误差(Mean Squared Error):<br>$$MSE=\frac{1}{N}\sum_{i=1}^N(y^{(i)}−f(x^{(i)}))$$<br>&nbsp;&nbsp;&nbsp;&nbsp;均方误差是指参数估计值与参数真值之差平方的期望值; MSE可以评价数据的变化程度，MSE的值越小，说明预测模型描述实验数据具有更好的精确度。（i表示第i个样本，N表示样本总数）通常做回归问题的代价函数。<br>（2）均方根误差:<br>$$RMSE=\sqrt{\frac{1}{N}\sum_{i=1}^N(y^{(i)}−f(x^{(i)}))}$$<br>&nbsp;&nbsp;&nbsp;&nbsp;均方根误差是均方误差的算术平方根，能够直观观测预测值与实际值的离散程度。通常做回归算法的性能指标。<br>（3）平均绝对误差（Mean Absolute Error）<br>$$MAE=\frac{1}{N}\sum_{i=1}^N|y^{(i)}−f(x^{(i)})|$$<br>&nbsp;&nbsp;&nbsp;&nbsp;平均绝对误差是绝对误差的平均值，平均绝对误差能更好地反映预测值误差的实际情况。通常做回归算法的性能指标。<br>（4）交叉熵代价函数（Cross Entry）<br>$$H(p,q)=−\frac{1}{N}\sum_{i=1}^NP(x^{(i)})logq(x^{(-i)})$$<br>&nbsp;&nbsp;&nbsp;&nbsp;交叉熵是用来评估当前训练得到的概率分布与真实分布的差异情况，减少交叉熵损失就是在提高模型的预测准确率。其中$p(x)$是指真实分布的概率，$q(x)$是模型通过数据计算出来的概率估计。<br>&nbsp;&nbsp;&nbsp;&nbsp;比如对于二分类模型的交叉熵代价函数（可参考逻辑回归一节）：<br>$$L(w,b)=−\frac{1}{N}\sum_{i=1}^N(y^{i}logf(x^{i})+(1−y^{i})log(1−f(x^{i})))$$<br>&nbsp;&nbsp;&nbsp;&nbsp;其中$f(x)$可以是sigmoid函数或深度学习中的其它激活函数，$y(i)∈(0,1)$。通常做分类问题的代价函数。<br>&nbsp;&nbsp;&nbsp;&nbsp;在求得损失后，就可以用其来反向调节网络的权值，以最小化损失，更好拟合所需函数。而神经网络的权值调节方法有两种，一种是从后往前调节，一种是从前往后调整参数，前者的典型代表就是“误差反向传播”，后者的代表就是“深度学习”。  </p>
<h3 id="误差反向传播"><a href="#误差反向传播" class="headerlink" title="误差反向传播"></a>误差反向传播</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;对于误差反向传播，简单地说就是首先随机设置初值，计算网络输出，然后根据输出和预期的差值，迭代计算，修改前面各层参数，直到网络收敛稳定。最有名的反向传播算法就是<strong>BP算法</strong>。</p>
<h4 id="BP算法"><a href="#BP算法" class="headerlink" title="BP算法"></a>BP算法</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;BP算法的本质是一个双向算法，其步骤为：（1）正向传播输入信息，实现分类功能；（2）反向传播误差，调整网络权值。<br>&nbsp;&nbsp;&nbsp;&nbsp;（1）正向传播输入信息：<br>&nbsp;&nbsp;&nbsp;&nbsp;什么是正向传播信息呢，就是将信号通过激活函数一层层向后传播，直到抵达输出层，前面提到的多层感知机就是实现了这样的功能。在这里我们假设激活函数为<strong>Sigmoid函数</strong>($f(x)=\frac{1}{1+e^{-x}}$)，其一是因为它将输出值域锁定在[0,1]之间，便于调节，其二因为它的求导形式非常简单（$f’(x)=f(x)(1-f(x))$）。<br>&nbsp;&nbsp;&nbsp;&nbsp;每一个神经元在正向传播过程中会负责两部分功能：（1）汇集各路连接带来的加权信息；（2）加权和信息在激活函数的作用下给出相应的输出。具体工作流程如下图所示： <img src="/2019/06/15/深度学习复习计划0-神经网络/1-2.png" alt="图片1-2"><br>&nbsp;&nbsp;&nbsp;&nbsp;那么对于一个完整的两层感知机，正向传播计算过程在下面给出图示和计算方法：  <img src="/2019/06/15/深度学习复习计划0-神经网络/1-3.png" alt="图片1-3"><br>&nbsp;&nbsp;&nbsp;&nbsp;对于节点$net_1$，正向计算过程为：<br>$$\begin{align}<br>f_{net1}(e)&amp;=f_1(x_{11}w_1+x_{21}w_2)    \\<br>&amp;=f_1(-1\times1+1\times-1)    \\<br>&amp;=f_1(-2)    \\<br>&amp;=\frac{1}{1+e^{-(-2)}}    \\<br>&amp;=0.12<br>\end{align}$$<br>&nbsp;&nbsp;&nbsp;&nbsp;同理可计算出:<br>$$\begin{align}<br>f_{net2}&amp;=0.5 \\<br>o_1&amp;=f_{net3}=f_3(w_{13}f_{net1}+w_{23}f_{net2})=0.65 \\<br>o_2&amp;=f_{net4}=f_4(w_{14}f_{net1}+w_{24}f_{net2})=0.59<br>\end{align}$$<br>&nbsp;&nbsp;&nbsp;&nbsp;至此，第一轮前向传播完成，实际输出的向量为$[0.65,0.59]$，假设期望输出为$[1,0]$，那么接下来要做的就是利用二者的误差，反向传播逐层调整网络参数，而在反向传播过程中，<strong>链式法则</strong>是最为有效的一种方式。  </p>
<h4 id="链式法则"><a href="#链式法则" class="headerlink" title="链式法则"></a>链式法则</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;在前面的前向传播过程中，所有的权值都是临时赋予的定值，而链式法则就是要调整这些权值到最优，以最小化代价函数来拟合训练数据。<br>&nbsp;&nbsp;&nbsp;&nbsp;那么对于各层神经元，暂时假设神经元没有激活函数，有：<br>$$\begin{align}<br>f_{net1}&amp;=x_1w_{11}+x_2w_{21} \\<br>f_{net2}&amp;=x_1w_{12}+x_2w_{22} \\<br>o_1&amp;=f_{net3}=w_{13}f_{net1}+w_{23}f_{net2} \\<br>&amp;=(x_1w_{11}+x_2w_{21})w_{13}+(x_1w_{12}+x_2w_{22})w_{23} \\<br>&amp;=x_1w_{11}w_{13}+x_2w_{21}w_{13}+x_1w_{12}w_{23}+x_2w_{22}w_{23}    \\<br>o_2&amp;=f_{net4}=w_{14}f_{net1}+w_{24}f_{net2}    \\<br>&amp;=(x_1w_{11}+x_2w_{21})w_{14}+(x_1w_{12}+x_2w_{22})w_{24} \\<br>&amp;=x_1w_{11}w_{14}+x_2w_{21}w_{14}+x_1w_{12}w_{24}+x_2w_{22}w_{24}<br>\end{align}$$<br>&nbsp;&nbsp;&nbsp;&nbsp;损失函数可以表示为：<br>$$L(w_{11},w_{12},…,w_{ij},…,w_{mn})=\frac{1}{2}(y_i-f_{net i}(w_{11},w_{12},…,w_{ij},…,w_{mn}))^2$$<br>&nbsp;&nbsp;&nbsp;&nbsp;其中$y_i$是预期输出向量，$f_{neti}$是实际输出向量。那么如何让损失函数最小化呢？常用的方法是<strong>梯度下降法</strong>。</p>
<h4 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;梯度下降法的基本流程如下：<br>（1）判断损失是否足够小，如果不是，计算损失函数的梯度；<br>（2）按照梯度的反方向移动一小步，来减少损失；<br>（3）从（1）开始继续循环。<br>&nbsp;&nbsp;&nbsp;&nbsp;通过这样的方法，加速损失函数值下降的速度。因此，上节提到的损失函数的梯度向量可以表示为：<br>$$\nabla L=\frac{\partial L}{\partial w_{11}},\frac{\partial L}{\partial w_{12}},…,\frac{\partial L}{\partial w_{mn}}$$<br>&nbsp;&nbsp;&nbsp;&nbsp;也就是说，要求出损失函数$L$对于每个权值$w_{ij}$的偏导数，而BP算法之所以经典，就是因为它非常适合求解这类“层层递进”偏导数。下面的一个例子会介绍两种求导方式，分别是“<strong>前向模式微分</strong>”和“<strong>反向模式微分</strong>”。</p>
<h5 id="前向模式微分"><a href="#前向模式微分" class="headerlink" title="前向模式微分"></a>前向模式微分</h5><p><img src="/2019/06/15/深度学习复习计划0-神经网络/1-4.png" alt="图片1-4"><br>&nbsp;&nbsp;&nbsp;&nbsp;对于直接相连的神经元，如a和c，利用加法规则或者乘法规则很容易求出导数，如$\frac{\partial c}{\partial a}$：<br>$$\frac{\partial c}{\partial w_{a}}=\frac{\partial (a+b)}{\partial a}=\frac{\partial a}{\partial a}+\frac{\partial b}{\partial a}=1$$<br>&nbsp;&nbsp;&nbsp;&nbsp;乘法规则为：<br>$$\frac{\partial uv}{\partial u}=u\frac{\partial v}{\partial u}+v\frac{\partial u}{\partial u}=v$$<br>&nbsp;&nbsp;&nbsp;&nbsp;那么对于间接相连的神经元，如a和e之间的导数如何求呢，就需要用到链式法则了。为了方便运算，这里假设$a=2,b=1$，那么容易知道c的变化率为1，因为$\frac{\partial c}{\partial a}=1$，并且容易推知，e的变化率为2，因为$\frac{\partial e}{\partial c}=d=2$，因此，a变化1，e的变化为：$1\times 2=2$，这就是所谓的“链式法则”，可以表示为：<br>$$ \frac{\partial e}{\partial a}=\frac{\partial e}{\partial c}\times \frac{\partial c}{\partial a}=d\times 1=2\times 1=2$$<br>&nbsp;&nbsp;&nbsp;&nbsp;同理，b对e的影响可以表示为：<br>$$ \frac{\partial e}{\partial b}=\frac{\partial cd}{\partial b}=d\frac{\partial c}{\partial b}+c\frac{\partial d}{\partial a}=d\times 1+c\times 1=2\times 1+3\times 1=5$$<br>&nbsp;&nbsp;&nbsp;&nbsp;因此可见b对e的影响分两路，一路是b影响c，c影响e；另一路是b影响d，d影响e。这就是链式法则的“路径加和”，这种同一条路径上面所有边相乘，然后将所有路径加和的方式，就是前向模式微分。  </p>
<h5 id="反向模式微分"><a href="#反向模式微分" class="headerlink" title="反向模式微分"></a>反向模式微分</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;前向模式微分虽然直观，但其存在一个很严重的问题，就是路径会被反复计算，如果有$n$个神经元，那么计算量将会达到$n^2$，在大型网络中是很难承受的。如下图所示：<br><img src="/2019/06/15/深度学习复习计划0-神经网络/1-5.png" alt="图片1-5"><br>&nbsp;&nbsp;&nbsp;&nbsp;根据路径加和原则得到前向模式微分X对Z的偏导数：<br>$$\frac{\partial Z}{\partial X}=(ad+ae+af)+(bd+be+bf)+(cd+ce+cf)$$</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;而反向模式微分求导的方式是这样的：<br>$$\frac{\partial Z}{\partial X}=(a+b+c)(d+e+f)$$<br>&nbsp;&nbsp;&nbsp;&nbsp;我们可以看出，前向模式微分方法强调的是输入节点对神经元的影响，因此在求导过程中，偏导数的分子变化，而分母锁定为偏导变量，保持不变。<br>&nbsp;&nbsp;&nbsp;&nbsp;相比而言，反向微分方式在求导方向上从输出端到输入端反向进行求导，它不再对每一条路径加权相乘后求和，而是对结点合并同类路径并分阶段求解。<br>&nbsp;&nbsp;&nbsp;&nbsp;如上图所示，反向模式微分先求Z对Z的影响，相当于$\frac{\partial Z}{\partial Z}=1$，然后求Y对Z节点的影响，就是求Z对Y的偏导：$\frac{\partial Z}{\partial Y}=\frac{\partial Z}{\partial Y}\frac{\partial Z}{\partial Z}=(d+e+f)\times 1$，最后求Z相对X的偏导：$\frac{\partial Z}{\partial X}=\frac{\partial Y}{\partial X}\frac{\partial Z}{\partial Y}\frac{\partial Z}{\partial S}=(a+b+c)(d+e+f)\times 1$。可见在反向求导过程中，偏导数的分子不变，而分母随着节点不同而变化。<br>&nbsp;&nbsp;&nbsp;&nbsp;还有一个细节是，当后向模式求导时，计算后一层单元的导数，需要利用前一层的求导结果，而前一层求导值已经得到，因此大大节省了计算成本。<br>&nbsp;&nbsp;&nbsp;&nbsp;这样，BP算法把网络权值纠错的运算量，从原来的神经元数目平方成正比，降低到和<strong>神经元数目成正比</strong>。</p>
<h4 id="基于随机梯度下降的BP算法"><a href="#基于随机梯度下降的BP算法" class="headerlink" title="基于随机梯度下降的BP算法"></a>基于随机梯度下降的BP算法</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;这里假设每一层激活函数都是sigmoid函数。设定对于样本$d$，随机梯度下降的损失函数为：<br>$$L_d(w)=\frac{1}{2}(t_d-o_d)^2$$<br>&nbsp;&nbsp;&nbsp;&nbsp;那么对于权值$w$，针对其损失函数$L_d(w)$，其梯度表示为$\frac{\partial {L_d}}{\partial w}$，同时为了调整步幅，设置权重$\eta \in (0,1]$，这样w的权重更新幅度为：<br>$$\Delta w=-\eta \frac{\partial {L_d}}{\partial w} $$<br>&nbsp;&nbsp;&nbsp;&nbsp;而对应的权值更新为：<br>$$w_{new} = w_{old}+\Delta w=w_{old} -\eta \frac{\partial {L_d}}{\partial w}$$</p>
<h5 id="输出层神经元权值训练"><a href="#输出层神经元权值训练" class="headerlink" title="输出层神经元权值训练"></a>输出层神经元权值训练</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;对于输出层神经元$j$，及其输出结果$o_j$，利用链式求导法则，得到：<br>$$\frac{\partial {L_d}}{\partial {w_j}}=\frac{\partial {L_d}}{\partial {o_j}}\frac{\partial {o_j}}{\partial {w_j}}$$<br>&nbsp;&nbsp;&nbsp;&nbsp;同时，对于$\frac{\partial {L_d}}{\partial {o_j}}$， 有：<br>$$\begin{align}<br>\frac{\partial {L_d}}{\partial {o_j}}&amp;=\frac{\partial }{\partial {o_j}}\frac{1}{2}(t_j-o_j)^2    \\<br>&amp;=\frac{1}{2}\times 2(t_j-o_j)\frac{\partial }{\partial {o_j}}    \\<br>&amp;=-(t_j - o_j)<br>\end{align}$$<br>&nbsp;&nbsp;&nbsp;&nbsp;对于$\frac{\partial {o_j}}{\partial {w_j}}$，还要考虑激活函数的加工处理，因此有：<br>$$ \frac{\partial {L_d}}{\partial {w_j}}=-(t_j-o_j)\frac{\partial {o_j}}{\partial {w_j}}=-(t_j-o_j)\sigma (w_j)’$$<br>&nbsp;&nbsp;&nbsp;&nbsp;又因为:<br>$$\begin{align}<br>\sigma (w_j)’&amp;=\frac{\partial {o_j}}{\partial {w_j}}=\frac{\partial {\sigma {w_j}}}{\partial {w_j}}    \\<br>&amp;=\sigma (w_j)(1-\sigma (w_j))    \\<br>&amp;=o_j(1 - o_j)<br>\end{align}$$<br>&nbsp;&nbsp;&nbsp;&nbsp;将上式全部代回，即可获得结果：<br>$$\frac{\partial {L_d}}{\partial {w_j}}=-(t_j-o_j)o_j(1-o_j)$$<br>&nbsp;&nbsp;&nbsp;&nbsp;那么最终的输出单元随机梯度下降法则为：<br>$$\Delta w=-\eta \frac{\partial {L_d}}{\partial w}=\eta o_j(1-o_j)l_jx_j$$<br>&nbsp;&nbsp;&nbsp;&nbsp;简记为：<br>$$\Delta w=\eta \delta_j^{(out)}x_j$$</p>
<h5 id="隐含层神经元权值训练"><a href="#隐含层神经元权值训练" class="headerlink" title="隐含层神经元权值训练"></a>隐含层神经元权值训练</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;对于第n隐含层的任意神经元$j$来说，它的直接下游神经元就是第n+1层的所有神经元。<br>&nbsp;&nbsp;&nbsp;&nbsp;因此，可以设定链式求导法则为：<br>$$\frac{\partial {L_d}}{\partial {w_j}}=\sum_{k\in w_{n+1}}\frac{\partial {L_d}}{\partial {w_{n+1}}}\frac{\partial {w_{n+1}}}{\partial {w_j}}$$</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;可以表示为:<br>$$\frac{\partial {L_d}}{\partial {w_j}}=\sum_{k\in w_{n+1}}-\delta_k\frac{\partial {w_k}}{\partial {l_j}}\frac{\partial {l_j}}{\partial {w_j}}$$</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;最终可以表示为:<br>$$\frac{\partial {L_d}}{\partial {w_j}}=-l_j(1-l_j)\sum_{k\in w_{n+1}}-\delta_kw_{kj}$$</p>
<h3 id="超参数调节"><a href="#超参数调节" class="headerlink" title="超参数调节"></a>超参数调节</h3><p>&nbsp;&nbsp;&nbsp;&nbsp; 神经网络的灵活性带来了太多的超参数需要调整，即使是简单的多层感知机，也有许多参数需要调整：层数、每层神经元数、激活函数类型、初始化权重等等……那么如何选择最优秀的超参数组合呢？<br>&nbsp;&nbsp;&nbsp;&nbsp; 可以使用交叉验证的网格搜索来查找正确的超参数，但是超参太多的话，网格搜索会很耗时，因此，使用随机搜索法的效果会好很多。另外，还可以使用如Oscar的工具，来更快查找超参数集。  </p>
<h4 id="隐藏层个数"><a href="#隐藏层个数" class="headerlink" title="隐藏层个数"></a>隐藏层个数</h4><p>&nbsp;&nbsp;&nbsp;&nbsp; 只要神经元足够多，仅有一个隐藏层的感知机都可以建模大部分复杂的函数，但是深层网络比浅层网络有更高的参数效率：深层网络可以用非常少的神经元来建模复杂函数，因此训练起来更加快速。  </p>
<h4 id="隐藏层的神经元数"><a href="#隐藏层的神经元数" class="headerlink" title="隐藏层的神经元数"></a>隐藏层的神经元数</h4><p>&nbsp;&nbsp;&nbsp;&nbsp; 通常来说，增加神经元数量比增加层数会带来更多的消耗，更简单的做法是使用比实际所需更多的层次和神经元，然后提前结束训练来避免过度拟合（以及使用其他正则化方法和dropout）。  </p>
<h3 id="梯度消失-爆炸问题"><a href="#梯度消失-爆炸问题" class="headerlink" title="梯度消失/爆炸问题"></a>梯度消失/爆炸问题</h3><p>&nbsp;&nbsp;&nbsp;&nbsp; 反向传播算法从输出层反向作用到输入层，在过程中传播误差梯度，一旦算法根据网络参数计算出来成本函数的梯度，就会根据梯度下降步骤来修正参数。<br>&nbsp;&nbsp;&nbsp;&nbsp; 但是梯度经常会随着算法进展到更低层时会变得原来越小，这样导致梯度下降在更低层网络连接权值更新方面基本没有改变，而且训练不会收敛到好的结果，这就是<strong>梯度消失</strong>问题。而在一些例子中发生相反的现象：梯度越来越大，导致很多层的权值疯狂增大，使得算法发散，这就是<strong>梯度爆炸</strong>问题（常出现在循环神经网络中）。  </p>
<h4 id="Xavier初始化和He初始化"><a href="#Xavier初始化和He初始化" class="headerlink" title="Xavier初始化和He初始化"></a>Xavier初始化和He初始化</h4><p>&nbsp;&nbsp;&nbsp;&nbsp; 当使用Sigmoid激活函数时，使用Xavier初始化，来避免梯度消失/爆炸问题；而使用ReLU激活函数（及其变种，包括ELU激活）的初始化方法，称为He初始化。  </p>
<h4 id="非饱和激活函数"><a href="#非饱和激活函数" class="headerlink" title="非饱和激活函数"></a>非饱和激活函数</h4><p>&nbsp;&nbsp;&nbsp;&nbsp; ReLU激活函数并不是完美的，它会出现dying ReLU问题：在训练过程中，一些神经元实际上已经死了，它只输出0，除非ReLU函数的梯度为0，并且输入为负，否则这个神经元就不会再重新开始工作。<br>&nbsp;&nbsp;&nbsp;&nbsp; 要解决这个问题，ReLU函数的变种如leaky ReLU、ELU等。  </p>
<h4 id="批量归一化"><a href="#批量归一化" class="headerlink" title="批量归一化"></a>批量归一化</h4><p>&nbsp;&nbsp;&nbsp;&nbsp; 尽管使用Xavier初始化和He初始化可以在训练初期降低梯度消失/爆炸问题，但是在训练过程中很难避免不再出现这些问题。因此，批量归一化（BN）技术提出来解决梯度消失/爆炸问题。</p>
<h4 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h4><p>丢弃学习（dropout）是指在深度学习网络训练过程中，对于神经网络单元，按照一定的概率p将其暂时从网络中丢弃，“丢弃学习”分为两个阶段：学习阶段和测试阶段。<br>在学习阶段，以概率p主动临时忽略部分隐藏节点，以减少网络大小，让神经网络学习数据的局部特征，以<strong>增强其泛化能力</strong>。<br>在测试阶段，将参与学习的节点和被隐藏的节点以一定概率p加权求和得到网络输出。</p>

      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/深度学习/" rel="tag"># 深度学习</a>
          
        </div>
      

      
      
        <div class="post-widgets">
        

        

        
          
          <div class="social_share">
            
            
               <div id="needsharebutton-postbottom">
                 <span class="btn">
                    <i class="fa fa-share-alt" aria-hidden="true"></i>
                 </span>
               </div>
            
          </div>
        
        </div>
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/06/04/机器学习复习计划6-聚类/" rel="next" title="机器学习复习计划6 - 聚类">
                <i class="fa fa-chevron-left"></i> 机器学习复习计划6 - 聚类
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/06/15/深度学习复习计划1-卷积神经网络/" rel="prev" title="深度学习复习计划1 - 卷积神经网络">
                深度学习复习计划1 - 卷积神经网络 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Shadowlight">
            
              <p class="site-author-name" itemprop="name">Shadowlight</p>
              <p class="site-description motion-element" itemprop="description">划水运动员，摸鱼爱好者的乐园。</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">18</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">3</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">3</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="https://github.com/shadowlightqywx" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:shadowlight_qywx@qq.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  
                </span>
              
            </div>
          

          
          

          
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#感知机"><span class="nav-number">1.</span> <span class="nav-text">感知机</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#学习规则"><span class="nav-number">1.1.</span> <span class="nav-text">学习规则</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#存在的问题"><span class="nav-number">1.2.</span> <span class="nav-text">存在的问题</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#多层感知机"><span class="nav-number">2.</span> <span class="nav-text">多层感知机</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#神经元的几何意义"><span class="nav-number">2.1.</span> <span class="nav-text">神经元的几何意义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#多层网络的构造方向"><span class="nav-number">2.2.</span> <span class="nav-text">多层网络的构造方向</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#损失函数"><span class="nav-number">3.</span> <span class="nav-text">损失函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#常见的损失函数"><span class="nav-number">3.1.</span> <span class="nav-text">常见的损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#常见的代价函数"><span class="nav-number">3.2.</span> <span class="nav-text">常见的代价函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#误差反向传播"><span class="nav-number">4.</span> <span class="nav-text">误差反向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#BP算法"><span class="nav-number">4.1.</span> <span class="nav-text">BP算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#链式法则"><span class="nav-number">4.2.</span> <span class="nav-text">链式法则</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#梯度下降法"><span class="nav-number">4.3.</span> <span class="nav-text">梯度下降法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#前向模式微分"><span class="nav-number">4.3.1.</span> <span class="nav-text">前向模式微分</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#反向模式微分"><span class="nav-number">4.3.2.</span> <span class="nav-text">反向模式微分</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#基于随机梯度下降的BP算法"><span class="nav-number">4.4.</span> <span class="nav-text">基于随机梯度下降的BP算法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#输出层神经元权值训练"><span class="nav-number">4.4.1.</span> <span class="nav-text">输出层神经元权值训练</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#隐含层神经元权值训练"><span class="nav-number">4.4.2.</span> <span class="nav-text">隐含层神经元权值训练</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#超参数调节"><span class="nav-number">5.</span> <span class="nav-text">超参数调节</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#隐藏层个数"><span class="nav-number">5.1.</span> <span class="nav-text">隐藏层个数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#隐藏层的神经元数"><span class="nav-number">5.2.</span> <span class="nav-text">隐藏层的神经元数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度消失-爆炸问题"><span class="nav-number">6.</span> <span class="nav-text">梯度消失/爆炸问题</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Xavier初始化和He初始化"><span class="nav-number">6.1.</span> <span class="nav-text">Xavier初始化和He初始化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#非饱和激活函数"><span class="nav-number">6.2.</span> <span class="nav-text">非饱和激活函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#批量归一化"><span class="nav-number">6.3.</span> <span class="nav-text">批量归一化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#dropout"><span class="nav-number">6.4.</span> <span class="nav-text">dropout</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
<div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>  &copy; 2018 – <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Shadowlight</span>

  

  
</div>











        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    
	
    

    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.4.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.4.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.4.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.4.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.4.1"></script>



  



  








  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  
  
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(function (item) {
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'fmBmjTAI27951oFefYnEFRB2-gzGzoHsz',
        appKey: 'JcQtVzu5N6qlksGuR72cvSR1',
        placeholder: '来呀！快活呀！',
        avatar:'mm',
        meta:guest,
        pageSize:'10' || 10,
        visitor: false
    });
  </script>



  





  

  
  <script>
    
    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();

      Counter('get', '/classes/Counter', { where: JSON.stringify({ url }) })
        .done(function ({ results }) {
          if (results.length > 0) {
            var counter = results[0];
            
            Counter('put', `/classes/Counter/${counter.objectId}`, JSON.stringify({ time: { "__op":"Increment", "amount":1 } }))
            
            .done(function () {
                  var $element = $(document.getElementById(url));
                  $element.find('.leancloud-visitors-count').text(counter.time + 1);
            })
            
            .fail(function ({ responseJSON }) {
                console.log('Failed to save Visitor num, with error message: ' + responseJSON.error);
            })
          } else {
            
              Counter('post', '/classes/Counter', JSON.stringify({ title: title, url: url, time: 1}))
                .done(function () {
                  var $element = $(document.getElementById(url));
                  $element.find('.leancloud-visitors-count').text(1);
                })
                .fail(function () {
                  console.log('Failed to create');
                });
            
          }
        })
      .fail(function ({ responseJSON }) {
        console.log('LeanCloud Counter Error:' + responseJSON.code + " " + responseJSON.error);
      });
    }
    

    $(function() {
      $.get('https://app-router.leancloud.cn/2/route?appId=' + "fmBmjTAI27951oFefYnEFRB2-gzGzoHsz")
        .done(function ({ api_server }) {
          var Counter = function (method, url, data) {
            return $.ajax({
              method: method,
              url: `https://${api_server}/1.1${url}`,
              headers: {
                'X-LC-Id': "fmBmjTAI27951oFefYnEFRB2-gzGzoHsz",
                'X-LC-Key': "JcQtVzu5N6qlksGuR72cvSR1",
                'Content-Type': 'application/json',
              },
              data: data,
            });
          };
          
          addCount(Counter);
          
        })
    });
  </script>



  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  
  
  
  <script src="/lib/needsharebutton/needsharebutton.js"></script>

  <script>
    
      pbOptions = {};
      
          pbOptions.iconStyle = "box";
      
          pbOptions.boxForm = "horizontal";
      
          pbOptions.position = "bottomCenter";
      
          pbOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
      
      new needShareButton('#needsharebutton-postbottom', pbOptions);
    
    
  </script>

  

  

  

  

  

  

<script src="/live2dw/lib/L2Dwidget.min.js?0c58a1486de42ac6cc1c59c7d98ae887"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/22.xmas.1.model.json"},"display":{"position":"left","width":130,"height":260},"mobile":{"show":false},"log":false});</script></body>
</html>
